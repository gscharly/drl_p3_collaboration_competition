{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "545e659e-03db-4b45-a2e0-da839bdf6bbe",
   "metadata": {},
   "source": [
    "# Learning algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d36a856c-f148-4e28-870f-ab498be8f53f",
   "metadata": {},
   "source": [
    "The proposed solution uses the Multi Agent Deep Deterministc Policy Gradient ([MADDPG](https://arxiv.org/abs/1706.02275)) algorithm to solve the Tennis environment. This algorithm provides a solution based on the DDPG algorithm for environments with multiple agents.\n",
    "\n",
    "As a recap, the DDPG algorithm combines Value-based and Policy-based methods by using 2 different networks:\n",
    "\n",
    "- Actor: models the policy. That is, it returns an action for a given state\n",
    "- Critic: models the Q-values. That is, it returns the expected return for a given pair of state-action\n",
    "\n",
    "MADDPG proposes a centralized approach for the Critic network and a decentralized approach for the Actor network. This means that each agent only uses local information to select the actions based on the policy, while the critic uses information from all agents. This extra information makes training easier and allows for centralized training with decentralized execution i.e. each agents takes actions based on their own observations of the environment.\n",
    "\n",
    "![maddpg](./maddpg.png)\n",
    "\n",
    "This version of the algorithm uses Experience replay, target networks and noise decay.\n",
    "\n",
    "## High level description\n",
    "\n",
    "### Neural networks architecture\n",
    "\n",
    "Each agent has 2 NNs: Actor and Critic. Each one has its target network associated, helping to reduce the correlation between the outputs and the NN weights. Multiple NN architectures have been tried, with this final structures:\n",
    "\n",
    "- Actor: 3 dense layers with decreasing dimensionality. Inputs are tensors of size 24 (states), and the NN dimensions are 256-128-2. The final output size is the actual action space size. Each layer has a batch normalization previous layer, that should reduce the training time. The activation functions are leaky RELUs to enable outputs smaller than 0. The final activation is a tanh function since the action space is continous.\n",
    "\n",
    "- Critic: 3 dense layers with decreasing dimensionality. The first layer has a batch normalization layer and a dense layer that accepts states and outputs tensors of dimension 256. These tensors are then concatenated with the actions, and 2 more dense layers are included, with dimensions 128-1. All activate functions are leaky RELUs. It is important to note that this network receives states and actions from all agents. \n",
    "\n",
    "### Algorithm\n",
    "\n",
    "- 2 different agents are initialized\n",
    "- For each agent, the Actor and Critic networks (local & target, 4 in total) are initialised with random weights\n",
    "- An initial state is drawn from the environment. We have to keep in mind that this environment has 2 agents, so the states and actions will belong to each of the agents\n",
    "- For each episode, an initial state is taken, and the algorithm can perform 2 actions: Sample and Learn\n",
    "- When sampling, the algorithm will choose an action using the Actor networks. Each agent receives its state, and outputs an action. The actions are concatenated and passed to the environment, receiving reward and thefollowing state. This 'experience' will be stored in a replay memory.\n",
    "- When selecting actions, Ornstein-Uhlenbeck funciton OU Noise is used to encourage exploration. A simple decay process is applied.\n",
    "- Each agent learns separately every N steps, and with multiple learning iterations every step. Each agent samples a batch of experiences from the replay memory, which includes states and actions from all agents. Then, it will update the Critic & Actor networks using the local & target networks, in a very similar way as the DDPG algorithm. The only difference is that the Critic uses information of all agents (states & actions)\n",
    "- The target networks are updated using a soft update approach."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec2b807f-0bb7-441a-a28d-fc94938c5645",
   "metadata": {},
   "source": [
    "# Experiments\n",
    "\n",
    "Before arriving to the final solution, multiple tests have been carried out.\n",
    "\n",
    "1. First, the DDPG agent implemented in the [second project](https://github.com/gscharly/drl_p2_continous_control) was used with a couple of modifications. The same Actor & Critic networks are used, and the experiences of each agent are used to update them. The environment was solved in around 2500 episodes, showing that the training process could probable be faster.\n",
    "2. Therefore, the MADDPG approach was followed to try to better adapt to the multi agent environment where both agents need to collaborate and compete. The environment was solved in less than 1000 episodes.\n",
    "3. Different NN architectures were tried out. Using batch normalization seems to help training time, and the best results were achieved using leaky relu activation functions.\n",
    "\n",
    "The DDPG agent's weights & results can be found under weights/ddpg. The MADDPG agents artifacts can be found under weights/maddpg."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db7a2401-adb4-4698-9f6d-4f77342f45dc",
   "metadata": {},
   "source": [
    "# Hyperparameters\n",
    "\n",
    "- Actor learning rate: 1e-3\n",
    "- Critic learning rate: 1e-3\n",
    "- Discount factor: 0.99\n",
    "- Tau (soft update): 1e-3\n",
    "- Batch size: 128\n",
    "- Learn every step, with 5 learning iterations in each step\n",
    "- Noise decay: .999"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e4d700-cfa1-4695-91cd-3eb4d8c768cf",
   "metadata": {},
   "source": [
    "# Rewards plot\n",
    "\n",
    "Both DDPG and MADDPG rewards plots are included. DDPG was trained for 3000 episodes and MADDPG for 1000 episodes.\n",
    "\n",
    "- Number of episodes required to solve the problem using DDPG: 2500.\n",
    "- Number of episodes required to solve the problem using MADDPG: 837.\n",
    "\n",
    "## DDPG\n",
    "![ddpg](../weights/ddpg/scores.png)\n",
    "\n",
    "## MADDPG\n",
    "![maddpg](../weights/maddpg/scores.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a092cf8a-430d-42d0-bb07-b27a75c464f0",
   "metadata": {},
   "source": [
    "# Agents playing!\n",
    "\n",
    "![agents](agent.gif \"agent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c19885c5-ae37-4848-a220-970482442cc9",
   "metadata": {},
   "source": [
    "# Ideas for future work\n",
    "\n",
    "- Implement [Prioritized Experience Replay](https://arxiv.org/abs/1511.05952). This can improve learning by increasing the probability of sampling important experiences.\n",
    "- Implement [Adaptive noise scaling](https://soeren-kirchner.medium.com/deep-deterministic-policy-gradient-ddpg-with-and-without-ornstein-uhlenbeck-process-e6d272adfc3). Instead of adding noise to the action, noise is added to the Actor's weight, which can lead to more consistent exploration and a richer set of behaviors. It is adaptive since the noise is increased/decreased based on the comparison between the original action and the action that would be selected when adding noise.\n",
    "- Further hyperparameter tuning: learning rates, noise decay..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlagents36",
   "language": "python",
   "name": "mlagents36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
